# HTML File Scraper

## Input
When run, this application takes a single directory location and list of URLs from a commmand line. It outputs two files: 

## Output

### Text File
The text file must output all traversed local pages sorted lexicographically by directory-starting with the site root. Each line should take the form "size page" where size is the cumulative size of all images (in MiB to 2 decimal places) on the page, and page is the local file path. The last line of the file shall contain the total size (i.e., sum of all page sizes). This format mirrors what is generated by the Linux du command (ie: du -h --max-depth 1).

### XMSX File
Spreadsheet file output with a single sheet titled "summary" with each row containing information for a single HTML file and all corresponding columns containing the detailed data information.

![Alt text](initial_uml.svg?raw=true "Title")

This application was initially developed as a group project for my CS350 course at Old Dominion University. I have removed all code that I did not write myself and have further developed the project. Original project repo [here](https://github.com/dbail014/Offline-Web-Analysis), with other contributor githubs below:
- johnwasikye
- dbail014
- aclar074

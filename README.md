# HTML File Scraper

## Input
When run, this application takes a single directory location and list of URLs from a commmand line. It outputs two files: 

## Output

### Text File
The text file must output all traversed local pages sorted lexicographically by directory-starting with the site root. Each line should take the form:

size page
where size is the cumulative size of all images (in MiB to 2 decimal places) on the page, and page is the local file path.

The last line of the file shall contain the total size (i.e., sum of all page sizes).

This format mirrors what is generated by the Linux du command. Using my CS 330 Review Example directory, I would run

du -h --max-depth 1
and receive, as output

12M     ./.git
5.9M    ./Review-01
14M     ./Review-02
17M     ./Review-03
3.3M    ./Review-04
2.1M    ./Review-05-Pointer-Review
28M     ./Review-06-CPP-Shapes
408K    ./Review-07-Java-Toys
15M     ./Review-09-Java-Shapes
2.7M    ./Review-10-GuiThread
124K    ./Review-08-UML-Sequence-Diagrams
3.9M    ./Review-12-Python3-Shapes
3.2M    ./Review-Dev
280K    ./Review-11-Threads
64K     ./Review-13-Python3-Toys
21M     ./build
4.0K    ./Scratch
76K     ./Review-14-Python3-LinkedList
126M    .

![Alt text](initial_uml.svg?raw=true "Title")

This application was initially developed as a group project for my CS350 course at Old Dominion University. I have removed all code that I did not write myself and have further developed the project. Original project repo [here](https://github.com/dbail014/Offline-Web-Analysis), with other contributor githubs below:
- johnwasikye
- dbail014
- aclar074
